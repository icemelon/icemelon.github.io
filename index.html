<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Haichen Shen's Home Page</title>

  <!-- Custom fonts for this theme -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

  <!-- Theme CSS -->
  <link href="css/freelancer.min.css" rel="stylesheet">
  <link href="css/theme.css" rel="stylesheet">

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-39033025-2', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg bg-secondary text-uppercase fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">Haichen's Homepage</a>
      <button class="navbar-toggler navbar-toggler-right text-uppercase font-weight-bold bg-primary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#publication">Publication</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#contact">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Masthead -->
  <header class="masthead bg-primary text-dark">
    <div class="container"> <!-- align-items-center flex-row"> -->
      <div class="row">
        <div class="col-md-0 col-lg-2">
        </div>
        <div class="col-md-4 col-lg-3 text-center">
          <!-- Masthead Avatar Image -->
          <img class="masthead-avatar avatar" src="img/profile.jpg" alt="">
        </div>
        <div class="col-md-7 col-lg-6 mt-2">
          <!-- Masthead Heading -->
          <h1 class="masthead-heading text-uppercase mb-3">Haichen Shen</h1>
          <p class="masthead-subheading font-weight-light h3">
            Co-founder<br>
            <a href="https://scroll.io">Scroll</a><br>
            haichen at scroll dot tech
          </p>
        </div>
      </div>
    </div>
  </header>

  <!-- About Section -->
  <section class="page-section mb-0" id="about">
    <div class="container">

      <!-- About Section Heading -->
      <h2 class="page-section-heading text-center text-uppercase mb-5">About</h2>

      <!-- About Section Content -->
      <div class="row">
        <div class="col-lg-9 m-auto">
        <p class="lead">
          I'm the co-founder at <a href="https://scroll.io">Scroll</a> and lead the engineering team.
          I'm also an open-source enthusiast and have been contributing to several open source projects,
          including <a href="https://github.com/appliedzkp/zkevm-circuits">zkEVM</a>,
          <a href="https://github.com/apache/tvm">Apache TVM</a>,
          <a href="https://github.com/awslabs/raf">RAF</a>.
        </p>
        <p class="lead">
          Previously, I worked at Amazon Web Services as a senior applied scientist on AI compilers.
          I received my Ph.D. in computer science from
          <a href="https://www.cs.washington.edu/">University of Washington</a>, advised by
          Professor <a href="http://www.cs.washington.edu/people/faculty/arvind">Arvind Krishnamurthy</a>
          and <a href="https://www.microsoft.com/en-us/research/people/matthaip/">Matthai Philipose</a>.
          I received my bachelor degree in computer science from
          Institute for Theoretical Computer Science at Tsinghua University.
        </p>
        </div>
      </div>
    </div>
  </section>

  <section class="page-section bg-primary text-dark mb-0" id="publication">
    <div class="container">
      <h2 class="page-section-heading text-center text-uppercase mb-5">Publication</h2>
      <div class="col-lg-9 m-auto">
        <ul class="paper">
          <li class="pubtitle">Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference</li>
          <li class="author"><strong>Haichen Shen*</strong>, Jared Roesch*, Zhi Chen, Wei Chen, Yong Wu, Mu Li, Vin Sharma, Zachary Tatlock, Yida Wang</li>
          <li class="conf">MLSys, Apr 2021</li>
          <li>
              <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_nimble').slideToggle('fast');return false;">
                  <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/nimble.pdf">
                  <i class="fa fa-file-pdf"></i> paper
              </a>
              <!-- <a class="btn btn-sm btn-secondary" href="papers/nexus.bib">
                  <i class="fa fa-file"></i> bibtex
              </a> -->
              <a class="btn btn-sm btn-secondary" href="slides/nimble_mlsys.pdf">
                  <i class="fa fa-file-pdf"></i> slides
              </a>
              <!-- <a class="btn btn-sm btn-secondary" href="https://sosp19.rcs.uwaterloo.ca/videos/D2-S2-P3.mp4">
                  <i class="fa fa-film"></i> talk
              </a> -->
              <!-- <a class="btn btn-sm btn-secondary" href="https://github.com/uwsampl/nexus">
                  <i class="fa fa-code"></i> code
              </a> -->
              </span>
              <div id="abstract_nimble" class="abstract">
                Modern deep neural networks increasingly make use of features such as control flow,
                dynamic data structures, and dynamic tensor shapes. Existing deep learning systems
                focus on optimizing and executing static neural networks which assume a pre-determined
                model architecture and input data shapes—assumptions that are violated by dynamic
                neural networks. Therefore, executing dynamic models with deep learning systems is
                currently both inflexible and sub-optimal, if not impossible. Optimizing dynamic
                neural networks is more challenging than static neural networks; optimizations must
                consider all possible execution paths and tensor shapes. This paper proposes Nimble,
                a high-performance and flexible system to optimize, compile, and execute dynamic
                neural networks on multiple platforms. Nimble handles model dynamism by introducing
                a dynamic type system, a set of dynamism-oriented optimizations, and a light-weight
                virtual machine runtime. Our evaluation demonstrates that Nimble outperforms
                existing solutions for dynamic neural networks by up to 20x on hardware platforms
                including Intel CPUs, ARM CPUs, and Nvidia GPUs.
              </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">Nexus: a GPU cluster engine for accelerating DNN-based video analysis</li>
          <li class="author"><strong>Haichen Shen</strong>, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, Ravi Sundaram</li>
          <li class="conf">SOSP, Oct 2019</li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_nexus').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/nexus.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/nexus.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
              <a class="btn btn-sm btn-secondary" href="slides/nexus_sosp.pdf">
                <i class="fa fa-file-pdf"></i> slides
              </a>
              <a class="btn btn-sm btn-secondary" href="https://sosp19.rcs.uwaterloo.ca/videos/D2-S2-P3.mp4">
                <i class="fa fa-film"></i> talk
              </a>
              <a class="btn btn-sm btn-secondary" href="https://github.com/uwsampl/nexus">
                <i class="fa fa-code"></i> code
              </a>
            </span>
            <div id="abstract_nexus" class="abstract">
              We address the problem of serving Deep Neural Networks (DNNs)
              efficiently from a cluster of GPUs. In order to realize the
              promise of very low-cost processing made by accelerators such as
              GPUs, it is essential to run them at sustained high
              utilization. Doing so requires cluster-scale resource management
              that performs detailed scheduling of GPUs, reasoning about groups
              of DNN invocations that need to be coscheduled, and moving from
              the conventional whole-DNN execution model to executing fragments
              of DNNs. Nexus is a fully implemented system that includes these
              innovations.  In large-scale case studies on 16 GPUs, when
              required to stay within latency constraints at least 99% of the
              time, Nexus can process requests at rates 1.8-12.7× higher than
              state of the art systems can. A long-running multi-application
              deployment stays within 84% of optimal utilization and, on a
              100-GPU cluster, violates latency SLOs on 0.27% of requests.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">TVM: An automated end-to-end optimizing compiler for deep learning<li>
          <li class="author">Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, <strong>Haichen Shen</strong>, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy</li>
          <li class="conf">OSDI, Sept 2018</li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_tvm').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/tvm.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/tvm.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
              <a class="btn btn-sm btn-secondary" href="https://tvm.ai/">
                <i class="fa fa-link"></i> project
              </a>
              <a class="btn btn-sm btn-secondary" href="https://github.com/apache/incubator-tvm">
                <i class="fa fa-code"></i> code
              </a>
            </span>
            <div id="abstract_tvm" class="abstract">
              There is an increasing need to bring machine learning to a wide
              diversity of hardware devices. Current frameworks rely on
              vendor-specific operator libraries and optimize for a narrow range
              of server-class GPUs. Deploying workloads to new platforms - such
              as mobile phones, embedded devices, and accelerators (e.g., FPGAs,
              ASICs) - requires significant manual effort. We propose TVM, a
              compiler that exposes graph-level and operator-level optimizations
              to provide performance portability to deep learning workloads
              across diverse hardware back-ends. TVM solves optimization
              challenges specific to deep learning, such as high-level operator
              fusion, mapping to arbitrary hardware primitives, and memory
              latency hiding. It also automates optimization of low-level
              programs to hardware characteristics by employing a novel,
              learning-based cost modeling method for rapid exploration of code
              optimizations. Experimental results show that TVM delivers
              performance across hardware back-ends that are competitive with
              state-of-the-art, hand-tuned libraries for low-power CPU, mobile
              GPU, and server-class GPUs. We also demonstrate TVM's ability to
              target new accelerator back-ends, such as the FPGA-based generic
              deep learning accelerator. The system is open sourced and in
              production use inside several major companies.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">Fast Video Classification via Adaptive Cascading of Deep Models</li>
          <li class="author"><strong>Haichen Shen</strong>, Seungyeop Han, Matthai Philipose, Arvind Krishnamurthy</li>
          <li class="conf">CVPR, July 2017 (spotlight)</li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_specialization').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/specialization.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/specialization.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
              <a class="btn btn-sm btn-secondary" href="slides/specialization_cvpr.pdf">
                <i class="fa fa-file-pdf"></i> slides
              </a>
              <a class="btn btn-sm btn-secondary" href="https://www.youtube.com/watch?v=Fg40LYLp_uc">
                <i class="fa fa-film"></i> talk
              </a>
              <a class="btn btn-sm btn-secondary" href="http://syslab.cs.washington.edu/research/specialization">
                <i class="fa fa-link"></i> project
              </a>
              <a class="btn btn-sm btn-secondary" href="https://github.com/icemelon/specialization">
                <i class="fa fa-code"></i> code
              </a>
            </span>
            <div id="abstract_specialization" class="abstract">
              Recent advances have enabled "oracle" classifiers that can
              classify across many classes and input distributions with high
              accuracy without retraining. However, these classifiers are
              relatively heavyweight, so that applying them to classify video is
              costly. We show that day-to-day video exhibits highly skewed class
              distributions over the short term, and that these distributions
              can be classified by much simpler models. We formulate the problem
              of detecting the short-term skews online and exploiting models
              based on it as a new sequential decision making problem dubbed the
              Online Bandit Problem, and present a new algorithm to solve
              it. When applied to recognizing faces in TV shows and movies, we
              realize end-toend classification speedups of 2.4-7.8×/2.6-11.2×
              (on GPU/CPU) relative to a state-of-the-art convolutional neural
              network, at competitive accuracy.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">MCDNN: An Approximation-Based Execution Framework for Deep Stream Processing Under Resource Constraints</li>
          <li class="author">Seungyeop Han*, <strong>Haichen Shen*</strong>, Matthai Philipose, Sharad Agarwal, Alec Wolman, Arvind Krishnamurthy</li>
          <li class="conf"> MobiSys, Jun. 2016 (*equally contributed) </li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_mcdnn').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/mcdnn.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/mcdnn.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
              <a class="btn btn-sm btn-secondary" href="slides/mcdnn_mobisys.pdf">
                <i class="fa fa-file-pdf"></i> slides
              </a>
              <a class="btn btn-sm btn-secondary" href="https://www.youtube.com/watch?v=gL8xdM6NCug&index=2&list=PL6jLuiS6wP5ZS2zV1ofDomdczF53Evm35">
                <i class="fa fa-film"></i> talk
              </a>
              <a class="btn btn-sm btn-secondary" href="https://www.youtube.com/watch?v=DJCO1mmDlno">
                <i class="fa fa-film"></i> 1-min video
              </a>
            </span>
            <div id="abstract_mcdnn" class="abstract">
              We consider applying computer vision to video on cloud-backed
              mobile devices using Deep Neural Networks (DNNs). The
              computational demands of DNNs are high enough that, without
              careful resource management, such applications strain device
              battery, wireless data, and cloud cost budgets. We pose the
              corresponding resource management problem, which we call
              Approximate Model Scheduling, as one of serving a stream of
              heterogeneous (i.e., solving multiple classification problems)
              requests under resource constraints. We present the design and
              implementation of an optimizing compiler and runtime scheduler to
              address this problem. Going beyond traditional resource
              allocators, we allow each request to be served approximately, by
              systematically trading off DNN classification accuracy for
              resource use, and remotely, by reasoning about on-device/cloud
              execution trade-offs. To inform the resource allocator, we
              characterize how several common DNNs, when subjected to
              state-of-the art optimizations, trade off accuracy for resource
              use such as memory, computation, and energy. The heterogeneous
              streaming setting is a novel one for DNN execution, and we
              introduce two new and powerful DNN optimizations that exploit
              it. Using the challenging continuous mobile vision domain as a
              case study, we show that our techniques yield significant
              reductions in resource usage and perform effectively over a broad
              range of operating conditions.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">Enhancing Mobile Apps To Use Sensor Hubs Without Programmer Effort</li>
          <li class="author"><strong>Haichen Shen</strong>, Aruna Balasubramanian, Anthony LaMarca, David Wetherall</li>
          <li class="conf">Ubicomp, Sept. 2015 <strong>(Best paper award, Gaetano Borriello best student paper award)</strong> </li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_mobilehub').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/mobilehub.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/mobilehub.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
              <a class="btn btn-sm btn-secondary" href="slides/mobilehub_ubicomp.pdf">
                <i class="fa fa-file-pdf"></i> slides
              </a>
              <a class="btn btn-sm btn-secondary" href="http://mobilehub.cs.washington.edu/">
                <i class="fa fa-link"></i> project
              </a>
            </span>
            <div id="abstract_mobilehub" class="abstract">
              Always-on continuous sensing apps drain the battery quickly because
              they prevent the main processor from sleeping. Instead, sensor hub
              hardware, available in many smartphones today, can run continuous
              sensing at lower power while keeping the main processor
              idle. However, developers have to divide functionality between the
              main processor and the sensor hub. We implement MobileHub, a system
              that automatically rewrites applications to leverage the sensor hub
              without additional programming effort. MobileHub uses a combination
              of dynamic taint tracking and machine learning to learn when it is
              safe to leverage the sensor hub without affecting application
              semantics. We implement MobileHub in Android and prototype a sensor
              hub on a 8-bit AVR micro-controller. We experiment with 20
              applications from Google Play. Our evaluation shows that MobileHub
              significantly reduces power consumption for continuous sensing apps.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">MetaSync: File Synchronization Across Multiple Untrusted Storage Services</li>
          <li class="author">Seungyeop Han, <strong>Haichen Shen</strong>, Taesoo Kim, Arvind Krishnamurthy, Thomas Anderson, David Wetherall</li>
          <li class="conf">USENIX ATC, July 2015</li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_metasync').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/metasync.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/metasync.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
              <a class="btn btn-sm btn-secondary" href="http://uwnetworkslab.github.io/metasync/">
                <i class="fa fa-link"></i> project
              </a>
            </span>
            <div id="abstract_metasync" class="abstract well">
              Cloud-based file synchronization services, such as Dropbox, are a
              worldwide resource for many millions of users.  However,
              individual services often have tight resource limits, suffer from
              temporary outages or even shutdowns, and sometimes silently
              corrupt or leak user data. <br>

              We design, implement, and evaluate MetaSync, a secure and reliable
              file synchronization service that uses multiple cloud
              synchronization services as untrusted storage providers. To make
              MetaSync work correctly, we devise a novel variant of Paxos that
              provides efficient and consistent updates on top of the unmodified
              APIs exported by existing services. Our system automatically
              redistributes files upon reconfiguration of providers. <br>

              Our evaluation shows that MetaSync provides low update latency and
              high update throughput while being more trustworthy and
              available. MetaSync outperforms its underlying cloud services by
              1.2-10× on three realistic workloads.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">Enable Flexible Spectrum Access with Spectrum Virtualization</li>
          <li class="author">Kun Tan, <strong>Haichen Shen</strong>, Jiansong Zhang, Yongguang Zhang</li>
          <li class="conf">DySPAN, Oct. 2012</li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_virtual_spectrum').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/virtual_spectrum.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/virtual_spectrum.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
            </span>
            <div id="abstract_virtual_spectrum" class="abstract">
              Enabling flexible spectrum access (FSA) in existing wireless
              networks is challenging due to the limited spectrum
              programmability - the ability to change spectrum properties of a
              signal to match an arbitrary frequency allocation. This paper
              argues that spectrum programmability can be separated from general
              wireless physical layer (PHY) modulation. Therefore, we can
              support flexible spectrum programmability by inserting a new
              spectrum virtualization layer (SVL) directly below traditional
              wireless PHY, and enable FSA for wireless networks without
              changing their PHY designs.<br>

              SVL provides a virtual baseband abstraction to wireless PHY, which
              is static, contiguous, with a desirable width defined by the
              PHY. At the sender side, SVL reshapes the modulated baseband
              signals into waveform that matches the dynamically allocated
              physical frequency bands - which can be of different width, or
              non-contiguous - while keeping the modulated information
              unchanged.  At the receiver side, SVL performs the inverse
              reshaping operation that collects the waveform from each physical
              band, and reconstructs the original modulated signals for PHY. All
              these reshaping operations are performed at the signal level and
              therefore SVL is agnostic and transparent to upper PHY. We have
              implemented a prototype of SVL on a software radio platform, and
              tested it with various wireless PHYs. Our experiments show SVL is
              flexible and effective to support FSA in existing wireless
              networks.
            </div>
          </li>
        </ul>
        <hr>
        <ul class="paper">
          <li class="pubtitle">Frame Retransmissions Considered Harmful: Improving Spectrum Efficiency Using Micro-ACKs</li>
          <li class="author">Jiansong Zhang, <strong>Haichen Shen</strong>, Kun Tan, Ranveer Chandra, Yongguang Zhang, Qian Zhang</li>
          <li class="conf">MobiCom, Aug. 2012</li>
          <li>
            <span class="links btn-group">
              <a class="btn btn-sm btn-secondary dropdown-toggle" href="javascript:void(0);" onclick="$('#abstract_uack').slideToggle('fast');return false;">
                <i class="fa fa-chevron-down"></i> abstract</a>
              <a class="btn btn-sm btn-secondary" href="papers/uack.pdf">
                <i class="fa fa-file-pdf"></i> paper
              </a>
              <a class="btn btn-sm btn-secondary" href="papers/uack.bib">
                <i class="fa fa-file"></i> bibtex
              </a>
            </span>
            <div id="abstract_uack" class="abstract">
              Retransmissions reduce the efficiency of data communication in
              wireless networks because of: (i) per-retransmission packet headers,
              (ii) contention overhead on every retransmission, and (iii)
              redundant bits in every retransmission. In fact, every
              retransmission nearly doubles the time to successfully deliver the
              packet. To improve spectrum efficiency in a lossy environment, we
              propose a new in-frame retransmission scheme using µACKs. Instead of
              waiting for the entire transmission to end before sending the ACK,
              the receiver sends smaller µACKs for every few symbols, on a
              separate narrow feedback channel. Based on these µACKs, the sender
              only retransmits the lost symbols after the last data symbol in the
              frame, thereby adaptively changing the frame size to ensure it is
              successfully delivered. We have implemented µACK on the Sora
              platform.  Experiments with our prototype validate the feasibility
              of symbollevel µACK. By significantly reducing the retransmistion
              overhead, the sender is able to aggressively use higher data rate
              for a lossy link. Both improve the overall network efficiency. Our
              experimental results from a controlled environment and an 9-node
              software radio testbed show that µACK can have up to 140% throughput
              gain over 802.11g and up to 60% gain over the best known
              retransmission scheme.
            </div>
          </li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Contact Section -->
  <section class="page-section" id="contact">
    <div class="container">

      <!-- Contact Section Heading -->
      <h2 class="page-section-heading text-center text-uppercase text-secondary mb-5">Contact Me</h2>

      <!-- Contact Section Form -->
      <div class="text-center">
        <a class="btn btn-outline-dark btn-social mx-1" href="https://www.linkedin.com/in/haichenshen/">
          <i class="fab fa-fw fa-linkedin-in"></i>
        </a>
        <a class="btn btn-outline-dark btn-social mx-1" href="https://github.com/icemelon">
          <i class="fab fa-fw fa-github"></i>
        </a>
        <a class="btn btn-outline-dark btn-social mx-1" href="https://twitter.com/shenhaichen">
          <i class="fab fa-fw fa-twitter"></i>
        </a>
      </div>
    </div>
  </section>

  <!-- Copyright Section -->
  <section class="copyright py-4 text-center text-white">
    <div class="container">
      <small>Copyright &copy; Haichen Shen 2022</small>
    </div>
  </section>

  <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
  <div class="scroll-to-top d-lg-none position-fixed ">
    <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top">
      <i class="fa fa-chevron-up"></i>
    </a>
  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Contact Form JavaScript -->
  <script src="js/jqBootstrapValidation.js"></script>
  <script src="js/contact_me.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/freelancer.min.js"></script>

</body>

</html>
